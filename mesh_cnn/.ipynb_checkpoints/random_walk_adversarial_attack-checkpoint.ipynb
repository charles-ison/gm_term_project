{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6596c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from options.test_options import TestOptions\n",
    "from data import DataLoader\n",
    "from models import create_model\n",
    "from models.layers.mesh_prepare import extract_features\n",
    "from util.writer import Writer\n",
    "from data.base_dataset import collate_fn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "510373a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attacked_model(model, dataset, writer):\n",
    "    writer.reset_counter()\n",
    "    attacked_model_outputs = []\n",
    "    \n",
    "    for i, data in enumerate(dataset):\n",
    "        model.set_input(data)\n",
    "        output, ncorrect, nexamples = model.test() \n",
    "        attacked_model_outputs.append(output)\n",
    "        writer.update_counter(ncorrect, nexamples)\n",
    "        \n",
    "        if(ncorrect == 0):\n",
    "            print(\"Label:  \" + str(data['label']))\n",
    "            print(\"files_name: \" + str(data['mesh'][0].filename))\n",
    "        \n",
    "    writer.print_acc(-1, writer.acc)\n",
    "    return attacked_model_outputs \n",
    "    \n",
    "def find_new_vertex_index(vertices_edges, edge_index, old_vertex_index):\n",
    "    for new_vertex_index, new_vertex_edges in enumerate(vertices_edges):\n",
    "            for new_edge_index in new_vertex_edges:\n",
    "                if(new_edge_index == edge_index and new_vertex_index != old_vertex_index):\n",
    "                    return new_vertex_index\n",
    "                \n",
    "def get_random_walk(mesh, random_walk_size):\n",
    "    walk_steps = 0\n",
    "    random_walk_vertices = []\n",
    "    random_walk_indices = []\n",
    "    random.seed(walk_steps)\n",
    "    vertex_index = random.randint(0, len(mesh.vs)-1)\n",
    "    \n",
    "    while walk_steps < random_walk_size: \n",
    "        random_walk_vertices.append(mesh.vs[vertex_index])\n",
    "        random_walk_indices.append(vertex_index)        \n",
    "        walk_steps += 1  \n",
    "        \n",
    "        vertex_edges = mesh.ve[vertex_index]\n",
    "        random.seed(walk_steps+1)\n",
    "        random_edge_index = random.randint(0, len(vertex_edges)-1)  \n",
    "        new_edge_index = vertex_edges[random_edge_index]\n",
    "        vertex_index = find_new_vertex_index(mesh.ve, new_edge_index, random_walk_indices[-1])\n",
    "        \n",
    "        #Prevents random walk from crossing over itself\n",
    "        count_of_vertex_edge_attempts = 0\n",
    "        walk_steps_backwards = 0\n",
    "        while(vertex_index in random_walk_indices):\n",
    "            if(count_of_vertex_edge_attempts >= len(vertex_edges)-1):\n",
    "                walk_steps_backwards += 1\n",
    "                go_back_to_index = walk_steps-walk_steps_backwards\n",
    "                \n",
    "                if(go_back_to_index == 0):\n",
    "                    # Trying again, mesh seems to be broken?\n",
    "                    return get_random_walk(mesh, random_walk_size)\n",
    "                \n",
    "                vertex_edges = mesh.ve[random_walk_indices[go_back_to_index]]\n",
    "                count_of_vertex_edge_attempts = 0\n",
    "                \n",
    "            random_edge_index = (random_edge_index + 1) % len(vertex_edges)\n",
    "            new_edge_index = vertex_edges[random_edge_index]\n",
    "            vertex_index = find_new_vertex_index(mesh.ve, new_edge_index, random_walk_indices[-1])\n",
    "            count_of_vertex_edge_attempts += 1        \n",
    "    \n",
    "    return torch.FloatTensor(random_walk_vertices), random_walk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b11d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imitating_network(imitating_nn, criterion, optimizer, random_walk_vertices, labels, attacked_model_outputs):\n",
    "\n",
    "    output = imitating_nn(random_walk_vertices)\n",
    "    loss = criterion(output, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step() \n",
    "    \n",
    "    _, predictions = torch.max(output.data, 1)\n",
    "    num_correct = (predictions == labels).sum().item()\n",
    "    \n",
    "    return output, loss.item(), num_correct\n",
    "\n",
    "def use_imitating_network_for_attack(imitating_nn, criterion, optimizer, random_walk_vertices, labels, attacked_model_outputs):\n",
    "\n",
    "    output = imitating_nn(random_walk_vertices)\n",
    "    loss = criterion(F.softmax(output, dim=1), F.softmax(attacked_model_outputs, dim=1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step() \n",
    "    \n",
    "    _, predictions = torch.max(output.data, 1)\n",
    "    num_correct = (predictions == labels).sum().item()\n",
    "    \n",
    "    return output, loss.item(), num_correct, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c93047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        combined = torch.cat((input, hidden), -1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "class Imitating_NN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Imitating_NN, self).__init__()\n",
    "        \n",
    "        self.scaling_factor = 10\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.first_linear = nn.Linear(input_size, 2*self.scaling_factor*input_size)\n",
    "        self.second_linear = nn.Linear(2*self.scaling_factor*input_size, self.scaling_factor*input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rnn = RNN(self.scaling_factor*input_size, self.scaling_factor*input_size, self.scaling_factor*input_size)\n",
    "        self.third_linear = nn.Linear(self.scaling_factor*input_size, 2*self.scaling_factor*input_size)\n",
    "        self.fourth_linear = nn.Linear(2*self.scaling_factor*input_size, output_size)\n",
    "        \n",
    "    def forward(self, random_walk_vertices):\n",
    "        \n",
    "        output = self.first_linear(random_walk_vertices)\n",
    "        output = self.second_linear(output)\n",
    "        output = self.relu(output)\n",
    "        \n",
    "        hidden = self.rnn.initHidden()\n",
    "        for step in output:\n",
    "            output, hidden = self.rnn(torch.reshape(step, (1, self.input_size*self.scaling_factor)), hidden)\n",
    "            \n",
    "        output = self.third_linear(output)\n",
    "        output = self.fourth_linear(output)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba35fae",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c07fac7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n",
      "loading the model from ./checkpoints/shrec16/latest_net.pth\n"
     ]
    }
   ],
   "source": [
    "testing_opt = TestOptions().parse()\n",
    "\n",
    "testing_opt.serial_batches = True\n",
    "dataloader = DataLoader(testing_opt)\n",
    "mesh_cnn = create_model(testing_opt)\n",
    "opt_writer = Writer(testing_opt)\n",
    "\n",
    "shift_weight = 0.3\n",
    "num_vertices_to_move = 5\n",
    "random_walk_size = 50\n",
    "num_categories = 30\n",
    "num_vertice_coordinates = 3\n",
    "imitating_nn = Imitating_NN(num_vertice_coordinates, num_categories)\n",
    "kld_criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "ce_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(imitating_nn.parameters(), lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "efa9fddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: -1, TEST ACC: [99.167 %]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Record testing accuracy before attack\n",
    "attacked_model_outputs = test_attacked_model(mesh_cnn, dataloader, opt_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "43ee32ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n",
      "epoch: 0, loss: 420.31246161460876 , accuracy: 0.0\n",
      "epoch: 1, loss: 413.1547076702118 , accuracy: 0.03333333333333333\n",
      "epoch: 2, loss: 412.74638080596924 , accuracy: 0.03333333333333333\n",
      "epoch: 3, loss: 411.7110643386841 , accuracy: 0.03333333333333333\n",
      "epoch: 4, loss: 410.71507263183594 , accuracy: 0.025\n",
      "epoch: 5, loss: 407.74311685562134 , accuracy: 0.025\n",
      "epoch: 6, loss: 400.38831305503845 , accuracy: 0.016666666666666666\n",
      "epoch: 7, loss: 411.4764380455017 , accuracy: 0.0\n",
      "epoch: 8, loss: 404.8008289337158 , accuracy: 0.041666666666666664\n",
      "epoch: 9, loss: 400.3096332550049 , accuracy: 0.041666666666666664\n",
      "epoch: 10, loss: 397.79956817626953 , accuracy: 0.016666666666666666\n",
      "epoch: 11, loss: 395.2716031074524 , accuracy: 0.03333333333333333\n",
      "epoch: 12, loss: 392.71112513542175 , accuracy: 0.025\n",
      "epoch: 13, loss: 390.10627150535583 , accuracy: 0.05\n",
      "epoch: 14, loss: 386.66722989082336 , accuracy: 0.05\n",
      "epoch: 15, loss: 382.64177644252777 , accuracy: 0.05\n",
      "epoch: 16, loss: 378.50582230091095 , accuracy: 0.06666666666666667\n",
      "epoch: 17, loss: 373.6908925771713 , accuracy: 0.06666666666666667\n",
      "epoch: 18, loss: 368.204827606678 , accuracy: 0.1\n",
      "epoch: 19, loss: 362.7437237203121 , accuracy: 0.1\n",
      "epoch: 20, loss: 357.0468496084213 , accuracy: 0.11666666666666667\n",
      "epoch: 21, loss: 352.9168250262737 , accuracy: 0.13333333333333333\n",
      "epoch: 22, loss: 348.75413942337036 , accuracy: 0.14166666666666666\n",
      "epoch: 23, loss: 344.14453180134296 , accuracy: 0.16666666666666666\n",
      "epoch: 24, loss: 339.4036199450493 , accuracy: 0.14166666666666666\n",
      "epoch: 25, loss: 334.3649701923132 , accuracy: 0.15\n",
      "epoch: 26, loss: 329.93179136514664 , accuracy: 0.15833333333333333\n",
      "epoch: 27, loss: 327.5189923495054 , accuracy: 0.15833333333333333\n",
      "epoch: 28, loss: 323.1076730340719 , accuracy: 0.15833333333333333\n",
      "epoch: 29, loss: 315.26585330814123 , accuracy: 0.16666666666666666\n",
      "epoch: 30, loss: 311.10169354081154 , accuracy: 0.18333333333333332\n",
      "epoch: 31, loss: 306.6386330127716 , accuracy: 0.2\n",
      "epoch: 32, loss: 302.41466998308897 , accuracy: 0.20833333333333334\n",
      "epoch: 33, loss: 298.1421890594065 , accuracy: 0.21666666666666667\n",
      "epoch: 34, loss: 294.23772560432553 , accuracy: 0.20833333333333334\n",
      "epoch: 35, loss: 289.26607973873615 , accuracy: 0.21666666666666667\n",
      "epoch: 36, loss: 284.6493860036135 , accuracy: 0.225\n",
      "epoch: 37, loss: 279.2680831104517 , accuracy: 0.24166666666666667\n",
      "epoch: 38, loss: 274.00799779593945 , accuracy: 0.2916666666666667\n",
      "epoch: 39, loss: 268.71959986165166 , accuracy: 0.30833333333333335\n",
      "epoch: 40, loss: 263.76851138845086 , accuracy: 0.31666666666666665\n",
      "epoch: 41, loss: 258.38662808761 , accuracy: 0.3333333333333333\n",
      "epoch: 42, loss: 252.5888475999236 , accuracy: 0.3416666666666667\n",
      "epoch: 43, loss: 247.1147758923471 , accuracy: 0.35833333333333334\n",
      "epoch: 44, loss: 241.75971361622214 , accuracy: 0.36666666666666664\n",
      "epoch: 45, loss: 236.2811330985278 , accuracy: 0.38333333333333336\n",
      "epoch: 46, loss: 231.1862153671682 , accuracy: 0.36666666666666664\n",
      "epoch: 47, loss: 226.22740503773093 , accuracy: 0.38333333333333336\n",
      "epoch: 48, loss: 221.51259356550872 , accuracy: 0.38333333333333336\n",
      "epoch: 49, loss: 216.47350820712745 , accuracy: 0.4083333333333333\n",
      "epoch: 50, loss: 210.40253963693976 , accuracy: 0.45\n",
      "epoch: 51, loss: 204.62254155613482 , accuracy: 0.4666666666666667\n",
      "epoch: 52, loss: 198.82795691024512 , accuracy: 0.48333333333333334\n",
      "epoch: 53, loss: 192.6574350837618 , accuracy: 0.5\n",
      "epoch: 54, loss: 186.7474527610466 , accuracy: 0.5083333333333333\n",
      "epoch: 55, loss: 181.02220366708934 , accuracy: 0.525\n",
      "epoch: 56, loss: 175.63658502791077 , accuracy: 0.525\n",
      "epoch: 57, loss: 170.1634100433439 , accuracy: 0.5583333333333333\n",
      "epoch: 58, loss: 165.71040185540915 , accuracy: 0.6\n",
      "epoch: 59, loss: 159.78361213766038 , accuracy: 0.5916666666666667\n",
      "epoch: 60, loss: 154.44362093228847 , accuracy: 0.6083333333333333\n",
      "epoch: 61, loss: 149.25951484963298 , accuracy: 0.6083333333333333\n",
      "epoch: 62, loss: 144.35872383788228 , accuracy: 0.625\n",
      "epoch: 63, loss: 139.5864503721241 , accuracy: 0.6333333333333333\n",
      "epoch: 64, loss: 135.16460321389604 , accuracy: 0.65\n",
      "epoch: 65, loss: 130.84971557359677 , accuracy: 0.65\n",
      "epoch: 66, loss: 126.80996561836218 , accuracy: 0.6666666666666666\n",
      "epoch: 67, loss: 122.74709210215951 , accuracy: 0.675\n",
      "epoch: 68, loss: 118.7342363873613 , accuracy: 0.6833333333333333\n",
      "epoch: 69, loss: 114.6605287349521 , accuracy: 0.6833333333333333\n",
      "epoch: 70, loss: 110.48920456385531 , accuracy: 0.7\n",
      "epoch: 71, loss: 106.80861096813169 , accuracy: 0.7166666666666667\n",
      "epoch: 72, loss: 102.8012719893668 , accuracy: 0.7166666666666667\n",
      "epoch: 73, loss: 99.42425282929617 , accuracy: 0.7333333333333333\n",
      "epoch: 74, loss: 95.70967905219277 , accuracy: 0.75\n",
      "epoch: 75, loss: 92.69109761341133 , accuracy: 0.7583333333333333\n",
      "epoch: 76, loss: 89.41586370020627 , accuracy: 0.7833333333333333\n",
      "epoch: 77, loss: 86.60210166871366 , accuracy: 0.7833333333333333\n",
      "epoch: 78, loss: 83.14430303359336 , accuracy: 0.7916666666666666\n",
      "epoch: 79, loss: 80.57770423963609 , accuracy: 0.7916666666666666\n",
      "epoch: 80, loss: 75.863437161315 , accuracy: 0.8\n",
      "epoch: 81, loss: 72.52626753516944 , accuracy: 0.8\n",
      "epoch: 82, loss: 68.5130573449423 , accuracy: 0.8333333333333334\n",
      "epoch: 83, loss: 64.39235546690176 , accuracy: 0.8583333333333333\n",
      "epoch: 84, loss: 61.28990952111778 , accuracy: 0.85\n",
      "epoch: 85, loss: 57.53966815589228 , accuracy: 0.8916666666666667\n",
      "epoch: 86, loss: 54.27672583571984 , accuracy: 0.8916666666666667\n",
      "epoch: 87, loss: 50.325386119249714 , accuracy: 0.9\n",
      "epoch: 88, loss: 49.68741150270213 , accuracy: 0.9\n",
      "epoch: 89, loss: 105.68231821222753 , accuracy: 0.7916666666666666\n",
      "epoch: 90, loss: 143.67142553432495 , accuracy: 0.6083333333333333\n",
      "epoch: 91, loss: 93.67620542409713 , accuracy: 0.75\n",
      "epoch: 92, loss: 55.971842885475326 , accuracy: 0.8916666666666667\n",
      "epoch: 93, loss: 42.15505687348792 , accuracy: 0.9333333333333333\n",
      "epoch: 94, loss: 37.1219919945857 , accuracy: 0.9416666666666667\n",
      "epoch: 95, loss: 33.20120474217094 , accuracy: 0.9416666666666667\n",
      "epoch: 96, loss: 30.54818935489157 , accuracy: 0.9416666666666667\n",
      "epoch: 97, loss: 28.704533174025357 , accuracy: 0.9416666666666667\n",
      "epoch: 98, loss: 27.270765802320398 , accuracy: 0.95\n",
      "epoch: 99, loss: 26.048153145464482 , accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Beginning Random Walk Attack\n",
    "dataloader = DataLoader(testing_opt)\n",
    "\n",
    "# Training Imitation Network\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    epoch_num_correct = 0\n",
    "    epoch_num_samples = 0;\n",
    "    for i, data in enumerate(dataloader):\n",
    "        mesh = data[\"mesh\"][0]\n",
    "        label = torch.from_numpy(data[\"label\"])\n",
    "        \n",
    "        random_walk_vertices, random_walk_indices = get_random_walk(mesh, random_walk_size)\n",
    "        \n",
    "        imitating_nn_output, loss, num_correct = train_imitating_network(imitating_nn, ce_criterion, optimizer, random_walk_vertices, label, attacked_model_outputs[i])\n",
    "        \n",
    "        epoch_num_samples += label.size(dim=-1)\n",
    "        epoch_loss += loss\n",
    "        epoch_num_correct += num_correct\n",
    "    \n",
    "    accuracy = epoch_num_correct / epoch_num_samples\n",
    "    print(\"epoch: \" + str(epoch) + \", loss: \" + str(epoch_loss), \", accuracy: \" + str(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9f6248d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n",
      "Label:  tensor([5])\n",
      "Prediction: tensor([14])\n",
      "new_file_name: datasets/random_walks/T497.obj\n",
      "Label:  tensor([5])\n",
      "Prediction: tensor([10])\n",
      "new_file_name: datasets/random_walks/T536.obj\n",
      "Label:  tensor([12])\n",
      "Prediction: tensor([19])\n",
      "new_file_name: datasets/random_walks/T587.obj\n"
     ]
    }
   ],
   "source": [
    "# Moving Vertices\n",
    "dataloader = DataLoader(testing_opt)\n",
    "\n",
    "def gradients_magnitude(vertices):\n",
    "    return vertices[1][0] ** 2 + vertices[1][1] ** 2 + vertices[1][2] ** 2\n",
    "    \n",
    "overridden_meshes = []\n",
    "for i, data in enumerate(dataloader):\n",
    "        mesh = data[\"mesh\"][0]\n",
    "        random_walk_vertices, random_walk_indices = get_random_walk(mesh, random_walk_size)\n",
    "        label = torch.from_numpy(data[\"label\"])\n",
    "    \n",
    "        random_walk_vertices.requires_grad = True\n",
    "        imitating_nn_output, loss, num_correct, pred = use_imitating_network_for_attack(imitating_nn, kld_criterion, optimizer, random_walk_vertices, label, attacked_model_outputs[i])\n",
    "        \n",
    "        gradients_dict = dict(zip(random_walk_indices, random_walk_vertices.grad))\n",
    "        gradients_dict = dict(sorted(gradients_dict.items(), key = gradients_magnitude))\n",
    "        \n",
    "        max_grad = torch.max(random_walk_vertices.grad.flatten(), 0)[0].item()\n",
    "        min_grad = torch.min(random_walk_vertices.grad.flatten(), 0)[0].item()\n",
    "        \n",
    "        num_vertices_changed = 0\n",
    "        while(num_vertices_changed < num_vertices_to_move):\n",
    "            \n",
    "            gradient_entry = gradients_dict.popitem()\n",
    "            index = gradient_entry[0]\n",
    "            gradients = gradient_entry[1]\n",
    "            \n",
    "            mesh.vs[index][0] += ((2 * shift_weight * (gradients[0]-min_grad) / (max_grad-min_grad)) - shift_weight)\n",
    "            mesh.vs[index][1] += ((2 * shift_weight * (gradients[1]-min_grad) / (max_grad-min_grad)) - shift_weight)\n",
    "            mesh.vs[index][2] += ((2 * shift_weight * (gradients[2]-min_grad) / (max_grad-min_grad)) - shift_weight)\n",
    "            \n",
    "            #mesh.vs[index][0] += gradients[0]\n",
    "            #mesh.vs[index][1] += gradients[1]\n",
    "            #mesh.vs[index][2] += gradients[2]\n",
    "    \n",
    "            num_vertices_changed += 1\n",
    "                \n",
    "        mesh.features = extract_features(mesh)\n",
    "        overridden_meshes.append(mesh)\n",
    "        new_file_name = \"datasets/random_walks/\" + mesh.filename\n",
    "        if(num_correct == 0):\n",
    "            print(\"Label:  \" + str(label))\n",
    "            print(\"Prediction: \" + str(pred))\n",
    "            print(\"new_file_name: \" + new_file_name)\n",
    "        mesh.export(file=new_file_name)\n",
    "\n",
    "dataloader.dataloader.dataset.override_meshes(overridden_meshes) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5bb4f4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: -1, TEST ACC: [80.0 %]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Record testing accuracy after random walk attack\n",
    "attacked_model_outputs = test_attacked_model(mesh_cnn, dataloader, opt_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "49ff849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n"
     ]
    }
   ],
   "source": [
    "# Random Pertubation Attack\n",
    "dataloader = DataLoader(testing_opt)\n",
    "overridden_meshes = []\n",
    "for i, data in enumerate(dataloader):\n",
    "    \n",
    "    mesh = data[\"mesh\"][0]\n",
    "    num_random_changes = 0\n",
    "    \n",
    "    while(num_random_changes < num_vertices_to_move):\n",
    "        \n",
    "        random_vertex_index = random.randint(0, len(mesh.vs)-1)    \n",
    "        mesh.vs[random_vertex_index][0] += random.uniform(-shift_weight, shift_weight)\n",
    "        mesh.vs[random_vertex_index][1] += random.uniform(-shift_weight, shift_weight)\n",
    "        mesh.vs[random_vertex_index][2] += random.uniform(-shift_weight, shift_weight)\n",
    "        num_random_changes += 1\n",
    "    \n",
    "    mesh.features = extract_features(mesh)\n",
    "    overridden_meshes.append(mesh)\n",
    "    new_file_name = \"datasets/random_pertubations/\" + mesh.filename\n",
    "    mesh.export(file=new_file_name)\n",
    "\n",
    "dataloader.dataloader.dataset.override_meshes(overridden_meshes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "955ac5db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [7]\n",
      "files_name: T576.obj\n",
      "Label:  [8]\n",
      "files_name: T598.obj\n",
      "Label:  [14]\n",
      "files_name: T105.obj\n",
      "Label:  [19]\n",
      "files_name: T404.obj\n",
      "Label:  [24]\n",
      "files_name: T435.obj\n",
      "Label:  [25]\n",
      "files_name: T492.obj\n",
      "Label:  [28]\n",
      "files_name: T540.obj\n",
      "Label:  [29]\n",
      "files_name: T461.obj\n",
      "Label:  [29]\n",
      "files_name: T581.obj\n",
      "Label:  [29]\n",
      "files_name: T586.obj\n",
      "epoch: -1, TEST ACC: [91.667 %]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Record testing accuracy after random pertubation attack\n",
    "attacked_model_outputs = test_attacked_model(mesh_cnn, dataloader, opt_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dbf252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
