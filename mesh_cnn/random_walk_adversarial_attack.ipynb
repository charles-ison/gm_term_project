{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6596c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from options.test_options import TestOptions\n",
    "from data import DataLoader\n",
    "from models import create_model\n",
    "from models.layers.mesh_prepare import extract_features\n",
    "from util.writer import Writer\n",
    "from data.base_dataset import collate_fn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510373a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attacked_model(model, dataset, writer):\n",
    "    writer.reset_counter()\n",
    "    attacked_model_outputs = []\n",
    "    \n",
    "    for i, data in enumerate(dataset):\n",
    "        model.set_input(data)\n",
    "        output, ncorrect, nexamples = model.test() \n",
    "        attacked_model_outputs.append(output)\n",
    "        writer.update_counter(ncorrect, nexamples)\n",
    "        \n",
    "    writer.print_acc(-1, writer.acc)\n",
    "    return attacked_model_outputs \n",
    "    \n",
    "def find_new_vertex_index(vertices_edges, edge_index, old_vertex_index):\n",
    "    for new_vertex_index, new_vertex_edges in enumerate(vertices_edges):\n",
    "            for new_edge_index in new_vertex_edges:\n",
    "                if(new_edge_index == edge_index and new_vertex_index != old_vertex_index):\n",
    "                    return new_vertex_index\n",
    "                \n",
    "def get_random_walk(mesh, random_walk_size):\n",
    "    walk_steps = 0\n",
    "    random_walk_vertices = []\n",
    "    random_walk_indices = []\n",
    "    vertex_index = random.randint(0, len(mesh.vs)-1)\n",
    "    \n",
    "    while walk_steps < random_walk_size: \n",
    "        random_walk_vertices.append(mesh.vs[vertex_index])\n",
    "        random_walk_indices.append(vertex_index)        \n",
    "        walk_steps += 1  \n",
    "        \n",
    "        vertex_edges = mesh.ve[vertex_index]\n",
    "        random_edge_index = random.randint(0, len(vertex_edges)-1)  \n",
    "        new_edge_index = vertex_edges[random_edge_index]\n",
    "        vertex_index = find_new_vertex_index(mesh.ve, new_edge_index, random_walk_indices[-1])\n",
    "        \n",
    "        #Prevents random walk from crossing over itself\n",
    "        count_of_vertex_edge_attempts = 0\n",
    "        walk_steps_backwards = 0\n",
    "        while(vertex_index in random_walk_indices):\n",
    "            if(count_of_vertex_edge_attempts >= len(vertex_edges)-1):\n",
    "                walk_steps_backwards += 1\n",
    "                go_back_to_index = walk_steps-walk_steps_backwards\n",
    "                \n",
    "                if(go_back_to_index == 0):\n",
    "                    # Trying again, mesh seems to be broken?\n",
    "                    return get_random_walk(mesh, random_walk_size)\n",
    "                \n",
    "                vertex_edges = mesh.ve[random_walk_indices[go_back_to_index]]\n",
    "                count_of_vertex_edge_attempts = 0\n",
    "                \n",
    "            random_edge_index = (random_edge_index + 1) % len(vertex_edges)\n",
    "            new_edge_index = vertex_edges[random_edge_index]\n",
    "            vertex_index = find_new_vertex_index(mesh.ve, new_edge_index, random_walk_indices[-1])\n",
    "            count_of_vertex_edge_attempts += 1        \n",
    "    \n",
    "    return torch.FloatTensor(random_walk_vertices), random_walk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b11d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imitating_network(imitating_nn, criterion, optimizer, random_walk_vertices, labels):\n",
    "\n",
    "    output = imitating_nn(random_walk_vertices)\n",
    "    loss = criterion(output, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step() \n",
    "    \n",
    "    _, predictions = torch.max(output.data, 1)\n",
    "    num_correct = (predictions == labels).sum().item()\n",
    "    \n",
    "    return output, loss.item(), num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c93047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        combined = torch.cat((input, hidden), -1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "class Imitating_NN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Imitating_NN, self).__init__()\n",
    "        \n",
    "        self.scaling_factor = 10\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.first_linear = nn.Linear(input_size, 2*self.scaling_factor*input_size)\n",
    "        self.second_linear = nn.Linear(2*self.scaling_factor*input_size, self.scaling_factor*input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.rnn = RNN(self.scaling_factor*input_size, self.scaling_factor*input_size, self.scaling_factor*input_size)\n",
    "        self.third_linear = nn.Linear(self.scaling_factor*input_size, 2*self.scaling_factor*input_size)\n",
    "        self.fourth_linear = nn.Linear(2*self.scaling_factor*input_size, output_size)\n",
    "        \n",
    "    def forward(self, random_walk_vertices):\n",
    "        \n",
    "        output = self.first_linear(random_walk_vertices)\n",
    "        output = self.second_linear(output)\n",
    "        output = self.relu(output)\n",
    "        \n",
    "        hidden = self.rnn.initHidden()\n",
    "        for step in output:\n",
    "            output, hidden = self.rnn(torch.reshape(step, (1, self.input_size*self.scaling_factor)), hidden)\n",
    "            \n",
    "        output = self.third_linear(output)\n",
    "        output = self.fourth_linear(output)\n",
    "            \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba35fae",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c07fac7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n",
      "loading the model from ./checkpoints/shrec16/latest_net.pth\n"
     ]
    }
   ],
   "source": [
    "testing_opt = TestOptions().parse()\n",
    "\n",
    "testing_opt.serial_batches = True\n",
    "dataloader = DataLoader(testing_opt)\n",
    "mesh_cnn = create_model(testing_opt)\n",
    "opt_writer = Writer(testing_opt)\n",
    "\n",
    "shift_weight = 0.2\n",
    "num_vertices_to_move = 5\n",
    "random_walk_size = 50\n",
    "num_categories = 30\n",
    "num_vertice_coordinates = 3\n",
    "imitating_nn = Imitating_NN(num_vertice_coordinates, num_categories)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(imitating_nn.parameters(), lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa9fddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: -1, TEST ACC: [99.167 %]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Record testing accuracy before attack\n",
    "attacked_model_outputs = test_attacked_model(mesh_cnn, dataloader, opt_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ee32ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ts/sy6vbbcs1vdbgtnds0713g6w0000gq/T/ipykernel_38491/1098266402.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  return torch.FloatTensor(random_walk_vertices), random_walk_indices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 417.96099519729614 , accuracy: 0.016666666666666666\n",
      "epoch: 1, loss: 412.9377884864807 , accuracy: 0.03333333333333333\n",
      "epoch: 2, loss: 412.79872155189514 , accuracy: 0.03333333333333333\n",
      "epoch: 3, loss: 411.5038158893585 , accuracy: 0.0\n",
      "epoch: 4, loss: 409.6977620124817 , accuracy: 0.008333333333333333\n",
      "epoch: 5, loss: 402.54399609565735 , accuracy: 0.041666666666666664\n",
      "epoch: 6, loss: 404.5584237575531 , accuracy: 0.025\n",
      "epoch: 7, loss: 401.5942211151123 , accuracy: 0.041666666666666664\n",
      "epoch: 8, loss: 393.63180553913116 , accuracy: 0.05\n",
      "epoch: 9, loss: 385.1892976760864 , accuracy: 0.05\n",
      "epoch: 10, loss: 374.5214728116989 , accuracy: 0.05\n",
      "epoch: 11, loss: 365.6240222454071 , accuracy: 0.041666666666666664\n",
      "epoch: 12, loss: 355.9369351863861 , accuracy: 0.075\n",
      "epoch: 13, loss: 351.3061738014221 , accuracy: 0.09166666666666666\n",
      "epoch: 14, loss: 345.26844650506973 , accuracy: 0.1\n",
      "epoch: 15, loss: 338.11372262239456 , accuracy: 0.1\n",
      "epoch: 16, loss: 335.9178787469864 , accuracy: 0.1\n",
      "epoch: 17, loss: 331.5175006389618 , accuracy: 0.125\n",
      "epoch: 18, loss: 325.6571955680847 , accuracy: 0.15\n",
      "epoch: 19, loss: 320.099467754364 , accuracy: 0.16666666666666666\n",
      "epoch: 20, loss: 313.9050472676754 , accuracy: 0.15\n",
      "epoch: 21, loss: 307.9891337454319 , accuracy: 0.18333333333333332\n",
      "epoch: 22, loss: 301.8905986249447 , accuracy: 0.19166666666666668\n",
      "epoch: 23, loss: 296.145610332489 , accuracy: 0.19166666666666668\n",
      "epoch: 24, loss: 290.38446804881096 , accuracy: 0.23333333333333334\n",
      "epoch: 25, loss: 284.7129592448473 , accuracy: 0.275\n",
      "epoch: 26, loss: 279.0082411766052 , accuracy: 0.275\n",
      "epoch: 27, loss: 273.5951338559389 , accuracy: 0.2833333333333333\n",
      "epoch: 28, loss: 267.9406218379736 , accuracy: 0.2833333333333333\n",
      "epoch: 29, loss: 262.6157555580139 , accuracy: 0.2916666666666667\n",
      "epoch: 30, loss: 257.10364006459713 , accuracy: 0.31666666666666665\n",
      "epoch: 31, loss: 251.4667586684227 , accuracy: 0.325\n",
      "epoch: 32, loss: 245.75937081873417 , accuracy: 0.3416666666666667\n",
      "epoch: 33, loss: 240.19283767789602 , accuracy: 0.36666666666666664\n",
      "epoch: 34, loss: 233.99779702723026 , accuracy: 0.39166666666666666\n",
      "epoch: 35, loss: 228.0256206318736 , accuracy: 0.44166666666666665\n",
      "epoch: 36, loss: 221.98074671253562 , accuracy: 0.44166666666666665\n",
      "epoch: 37, loss: 216.07681290060282 , accuracy: 0.44166666666666665\n",
      "epoch: 38, loss: 210.92348302528262 , accuracy: 0.45\n",
      "epoch: 39, loss: 205.73263647779822 , accuracy: 0.4583333333333333\n",
      "epoch: 40, loss: 201.2453142479062 , accuracy: 0.45\n",
      "epoch: 41, loss: 197.50187880173326 , accuracy: 0.4583333333333333\n",
      "epoch: 42, loss: 193.6856037005782 , accuracy: 0.475\n",
      "epoch: 43, loss: 190.33544560149312 , accuracy: 0.4666666666666667\n",
      "epoch: 44, loss: 187.73804563470185 , accuracy: 0.475\n",
      "epoch: 45, loss: 184.99631307274103 , accuracy: 0.49166666666666664\n",
      "epoch: 46, loss: 182.31335361488163 , accuracy: 0.5083333333333333\n",
      "epoch: 47, loss: 177.13487754575908 , accuracy: 0.5333333333333333\n",
      "epoch: 48, loss: 166.01961541501805 , accuracy: 0.5583333333333333\n",
      "epoch: 49, loss: 162.88946652971208 , accuracy: 0.575\n",
      "epoch: 50, loss: 155.2681402869057 , accuracy: 0.6083333333333333\n",
      "epoch: 51, loss: 157.60423979267944 , accuracy: 0.575\n",
      "epoch: 52, loss: 146.19548969252355 , accuracy: 0.6\n",
      "epoch: 53, loss: 147.73313079123182 , accuracy: 0.6083333333333333\n",
      "epoch: 54, loss: 140.58152786203937 , accuracy: 0.6416666666666667\n",
      "epoch: 55, loss: 133.7179603727709 , accuracy: 0.65\n",
      "epoch: 56, loss: 138.35087179369293 , accuracy: 0.6333333333333333\n",
      "epoch: 57, loss: 146.31596407825418 , accuracy: 0.5833333333333334\n",
      "epoch: 58, loss: 154.97623530798774 , accuracy: 0.625\n",
      "epoch: 59, loss: 128.4212811394118 , accuracy: 0.6666666666666666\n",
      "epoch: 60, loss: 120.4585639648667 , accuracy: 0.7\n",
      "epoch: 61, loss: 113.32906327754603 , accuracy: 0.7\n",
      "epoch: 62, loss: 109.99512740661157 , accuracy: 0.7083333333333334\n",
      "epoch: 63, loss: 103.17088249674998 , accuracy: 0.7333333333333333\n",
      "epoch: 64, loss: 106.66602797905216 , accuracy: 0.725\n",
      "epoch: 65, loss: 126.5602022018279 , accuracy: 0.675\n",
      "epoch: 66, loss: 111.09572394688075 , accuracy: 0.6833333333333333\n",
      "epoch: 67, loss: 97.09265336881072 , accuracy: 0.7166666666666667\n",
      "epoch: 68, loss: 110.12143395876035 , accuracy: 0.7\n",
      "epoch: 69, loss: 92.95228470425332 , accuracy: 0.7416666666666667\n",
      "epoch: 70, loss: 91.521214858647 , accuracy: 0.7416666666666667\n",
      "epoch: 71, loss: 87.22206293172621 , accuracy: 0.7916666666666666\n",
      "epoch: 72, loss: 76.60279769891713 , accuracy: 0.7916666666666666\n",
      "epoch: 73, loss: 74.66653639500873 , accuracy: 0.7833333333333333\n",
      "epoch: 74, loss: 74.10602714888228 , accuracy: 0.7833333333333333\n",
      "epoch: 75, loss: 68.17348199813568 , accuracy: 0.8166666666666667\n",
      "epoch: 76, loss: 67.23268076586464 , accuracy: 0.8083333333333333\n",
      "epoch: 77, loss: 64.43807534289772 , accuracy: 0.8333333333333334\n",
      "epoch: 78, loss: 96.12926394602096 , accuracy: 0.7166666666666667\n",
      "epoch: 79, loss: 157.35411848460717 , accuracy: 0.5916666666666667\n",
      "epoch: 80, loss: 92.8649920391267 , accuracy: 0.7666666666666667\n",
      "epoch: 81, loss: 88.45334338127213 , accuracy: 0.75\n",
      "epoch: 82, loss: 72.87152283675005 , accuracy: 0.775\n",
      "epoch: 83, loss: 56.5952866564856 , accuracy: 0.8583333333333333\n",
      "epoch: 84, loss: 50.570706091461375 , accuracy: 0.9\n",
      "epoch: 85, loss: 46.38816276883745 , accuracy: 0.9\n",
      "epoch: 86, loss: 43.42470342530669 , accuracy: 0.9166666666666666\n",
      "epoch: 87, loss: 42.055156856482995 , accuracy: 0.9\n",
      "epoch: 88, loss: 42.43547849763581 , accuracy: 0.925\n",
      "epoch: 89, loss: 45.134513405084135 , accuracy: 0.9083333333333333\n",
      "epoch: 90, loss: 126.71481315594059 , accuracy: 0.6916666666666667\n",
      "epoch: 91, loss: 99.84170534369332 , accuracy: 0.75\n",
      "epoch: 92, loss: 67.99135891873378 , accuracy: 0.8083333333333333\n",
      "epoch: 93, loss: 43.569320996951205 , accuracy: 0.9166666666666666\n",
      "epoch: 94, loss: 36.27449930821237 , accuracy: 0.95\n",
      "epoch: 95, loss: 32.58353139816335 , accuracy: 0.9583333333333334\n",
      "epoch: 96, loss: 29.339696819895828 , accuracy: 0.975\n",
      "epoch: 97, loss: 27.14889082444153 , accuracy: 0.9833333333333333\n",
      "epoch: 98, loss: 25.21389049087758 , accuracy: 0.9833333333333333\n",
      "epoch: 99, loss: 25.608192915387008 , accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Beginning Random Walk Attack\n",
    "dataloader = DataLoader(testing_opt)\n",
    "\n",
    "# Training Imitation Network\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    epoch_num_correct = 0\n",
    "    epoch_num_samples = 0;\n",
    "    for i, data in enumerate(dataloader):\n",
    "        mesh = data[\"mesh\"][0]\n",
    "        label = torch.from_numpy(data[\"label\"])\n",
    "        \n",
    "        random_walk_vertices, random_walk_indices = get_random_walk(mesh, random_walk_size)\n",
    "        \n",
    "        #TODO: combe back and use random walk here\n",
    "        imitating_nn_output, loss, num_correct = train_imitating_network(imitating_nn, criterion, optimizer, torch.FloatTensor(mesh.vs), label)\n",
    "        \n",
    "        epoch_num_samples += label.size(dim=-1)\n",
    "        epoch_loss += loss\n",
    "        epoch_num_correct += num_correct\n",
    "    \n",
    "    accuracy = epoch_num_correct / epoch_num_samples\n",
    "    print(\"epoch: \" + str(epoch) + \", loss: \" + str(epoch_loss), \", accuracy: \" + str(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9f6248d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n"
     ]
    }
   ],
   "source": [
    "# Moving Vertices\n",
    "dataloader = DataLoader(testing_opt)\n",
    "\n",
    "def gradients_magnitude(vertices):\n",
    "    return vertices[1][0] ** 2 + vertices[1][1] ** 2 + vertices[1][2] ** 2\n",
    "    \n",
    "overridden_meshes = []\n",
    "for i, data in enumerate(dataloader):\n",
    "        mesh = data[\"mesh\"][0]\n",
    "        random_walk_vertices, random_walk_indices = get_random_walk(mesh, random_walk_size)\n",
    "        label = torch.from_numpy(data[\"label\"])\n",
    "    \n",
    "        #random_walk_vertices.requires_grad = True\n",
    "        mesh_vs = torch.FloatTensor(mesh.vs)\n",
    "        mesh_vs.requires_grad = True\n",
    "        \n",
    "        #TODO: combe back and use random walk here\n",
    "        imitating_nn_output, loss, num_correct = train_imitating_network(imitating_nn, criterion, optimizer, mesh_vs, label)\n",
    "        \n",
    "        gradients_dict = {index: gradients for index, gradients in enumerate(mesh_vs.grad)}\n",
    "        gradients_dict = dict(sorted(gradients_dict.items(), key = gradients_magnitude))\n",
    "        \n",
    "        max_grad = torch.max(mesh_vs.grad.flatten(), 0)[0].item()\n",
    "        min_grad = torch.min(mesh_vs.grad.flatten(), 0)[0].item()\n",
    "        \n",
    "        num_vertices_changed = 0\n",
    "        while(num_vertices_changed < num_vertices_to_move):\n",
    "            \n",
    "            gradient_entry = gradients_dict.popitem()\n",
    "            index = gradient_entry[0]\n",
    "            gradients = gradient_entry[1]\n",
    "            \n",
    "            #mesh.vs[index][0] += ((2 * shift_weight * (gradients[0]-min_grad) / (max_grad-min_grad)) - shift_weight)\n",
    "            #mesh.vs[index][1] += ((2 * shift_weight * (gradients[1]-min_grad) / (max_grad-min_grad)) - shift_weight)\n",
    "            #mesh.vs[index][2] += ((2 * shift_weight * (gradients[2]-min_grad) / (max_grad-min_grad)) - shift_weight)\n",
    "            \n",
    "            mesh.vs[index][0] += gradients[0]\n",
    "            mesh.vs[index][1] += gradients[1]\n",
    "            mesh.vs[index][2] += gradients[2]\n",
    "            \n",
    "            num_vertices_changed += 1\n",
    "            \n",
    "        mesh.features = extract_features(mesh)\n",
    "        overridden_meshes.append(mesh)\n",
    "        new_file_name = \"datasets/random_walks/\" + mesh.filename\n",
    "        mesh.export(file=new_file_name)\n",
    "\n",
    "dataloader.dataloader.dataset.override_meshes(overridden_meshes) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bb4f4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: -1, TEST ACC: [38.333 %]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Record testing accuracy after random walk attack\n",
    "attacked_model_outputs = test_attacked_model(mesh_cnn, dataloader, opt_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49ff849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded mean / std from cache\n"
     ]
    }
   ],
   "source": [
    "# Random Pertubation Attack\n",
    "dataloader = DataLoader(testing_opt)\n",
    "overridden_meshes = []\n",
    "for i, data in enumerate(dataloader):\n",
    "    \n",
    "    mesh = data[\"mesh\"][0]\n",
    "    num_random_changes = 0\n",
    "    \n",
    "    while(num_random_changes < num_vertices_to_move):\n",
    "        \n",
    "        random_vertex_index = random.randint(0, len(mesh.vs)-1)    \n",
    "        mesh.vs[random_vertex_index][0] += random.uniform(-shift_weight, shift_weight)\n",
    "        mesh.vs[random_vertex_index][1] += random.uniform(-shift_weight, shift_weight)\n",
    "        mesh.vs[random_vertex_index][2] += random.uniform(-shift_weight, shift_weight)\n",
    "        num_random_changes += 1\n",
    "    \n",
    "    mesh.features = extract_features(mesh)\n",
    "    overridden_meshes.append(mesh)\n",
    "    new_file_name = \"datasets/random_pertubations/\" + mesh.filename\n",
    "    mesh.export(file=new_file_name)\n",
    "\n",
    "dataloader.dataloader.dataset.override_meshes(overridden_meshes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "955ac5db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: -1, TEST ACC: [96.667 %]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Record testing accuracy after random pertubation attack\n",
    "attacked_model_outputs = test_attacked_model(mesh_cnn, dataloader, opt_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dbf252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
